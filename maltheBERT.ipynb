{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A basic classifier based on the transformers (https://github.com/huggingface/transformers) \n",
    "library. It loads a masked language model (by default mBERT), and adds a linear layer for\n",
    "prediction. Example usage:\n",
    "python3 bert-topic.py topic-data/train.txt topic-data/dev.txt\n",
    "\"\"\"\n",
    "from typing import List, Dict\n",
    "import codecs\n",
    "import torch\n",
    "import sys\n",
    "import myutils\n",
    "import helpers\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for consistency\n",
    "torch.manual_seed(8446)\n",
    "# Set some constants\n",
    "MLM = 'distilbert-base-cased'\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 3\n",
    "# We have an UNK label for robustness purposes, it makes it easier to run on\n",
    "# data with other labels, or without labels.\n",
    "UNK = \"[UNK]\"\n",
    "MAX_TRAIN_SENTS=64\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModel(torch.nn.Module):\n",
    "    def __init__(self, nlabels: int, mlm: str):\n",
    "        \"\"\"\n",
    "        Model for classification with transformers.\n",
    "        The architecture of this model is simple, we just have a transformer\n",
    "        based language model, and add one linear layer to converts it output\n",
    "        to our prediction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlabels : int\n",
    "            Vocabulary size of output space (i.e. number of labels)\n",
    "        mlm : str\n",
    "            Name of the transformers language model to use, can be found on:\n",
    "            https://huggingface.co/models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The transformer model to use\n",
    "        self.mlm = AutoModel.from_pretrained(mlm)\n",
    "\n",
    "        # Find the size of the output of the masked language model\n",
    "        if hasattr(self.mlm.config, 'hidden_size'):\n",
    "            self.mlm_out_size = self.mlm.config.hidden_size\n",
    "        elif hasattr(self.mlm.config, 'dim'):\n",
    "            self.mlm_out_size = self.mlm.config.dim\n",
    "        else: # if not found, guess\n",
    "            self.mlm_out_size = 768\n",
    "\n",
    "        # Create prediction layer\n",
    "        self.hidden_to_label = torch.nn.Linear(self.mlm_out_size, nlabels)\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : torch.tensor\n",
    "            Tensor with wordpiece indices. shape=(batch_size, max_sent_len).\n",
    "        Returns\n",
    "        -------\n",
    "        output_scores : torch.tensor\n",
    "            ?. shape=(?,?)\n",
    "        \"\"\"\n",
    "        # Run transformer model on input\n",
    "        mlm_out = self.mlm(input)\n",
    "\n",
    "        # Keep only the last layer: shape=(batch_size, max_len, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out.last_hidden_state\n",
    "        # Keep only the output for the first ([CLS]) token: shape=(batch_size, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out[:,:1,:].squeeze()\n",
    "\n",
    "        # Matrix multiply to get scores for each label: shape=(?,?)\n",
    "        output_scores = self.hidden_to_label(mlm_out)\n",
    "\n",
    "        return output_scores\n",
    "\n",
    "    def run_eval(self, text_batched: List[torch.tensor], labels_batched: List[torch.tensor]):\n",
    "        \"\"\"\n",
    "        Run evaluation: predict and score\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_batched : List[torch.tensor]\n",
    "            list with batches of text, containing wordpiece indices.\n",
    "        labels_batched : List[torch.tensor]\n",
    "            list with batches of labels (converted to ints).\n",
    "        model : torch.nn.module\n",
    "            The model to use for prediction.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            accuracy of model on labels_batches given feats_batches\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for sents, labels in zip(text_batched, labels_batched):\n",
    "            output_scores = self.forward(sents)\n",
    "            pred_labels = torch.argmax(output_scores, 1)\n",
    "            for gold_label, pred_label in zip(labels, pred_labels):\n",
    "                total += 1\n",
    "                if gold_label.item() == pred_label.item():\n",
    "                    match+= 1\n",
    "        return(match/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n"
     ]
    }
   ],
   "source": [
    "print('reading data...')\n",
    "train_data = helpers.load_conll(\"data/en_ewt_nn_train.conll\") # Format: List[([sentence],[named entity])]\n",
    "train_text = [sent for sent, _ in train_data]\n",
    "train_labels = [labels for _, labels in train_data]\n",
    "assert len(train_text) == len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m id2label, label2id \u001b[39m=\u001b[39m myutils\u001b[39m.\u001b[39;49mlabels2lookup(train_labels, UNK)\n\u001b[1;32m      2\u001b[0m NLABELS \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(id2label)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(train_labels)\n",
      "File \u001b[0;32m~/Documents/ITU/Fourth-semester/Second-year-project/NLP-second-year-project/myutils.py:127\u001b[0m, in \u001b[0;36mlabels2lookup\u001b[0;34m(labels, PAD)\u001b[0m\n\u001b[1;32m    125\u001b[0m label2id \u001b[39m=\u001b[39m {PAD: \u001b[39m0\u001b[39m}\n\u001b[1;32m    126\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m labels:\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mif\u001b[39;00m label \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m label2id:\n\u001b[1;32m    128\u001b[0m         label2id[label] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(label2id)\n\u001b[1;32m    129\u001b[0m         id2label\u001b[39m.\u001b[39mappend(label)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "id2label, label2id = myutils.labels2lookup(train_labels, UNK)\n",
    "NLABELS = len(id2label)\n",
    "print(train_labels)\n",
    "print(label2id)\n",
    "train_labels = [label2id[label] for label in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2label, label2id = myutils.labels2lookup(train_labels, UNK)\n",
    "NLABELS = len(id2label)\n",
    "print(train_labels)\n",
    "print(label2id)\n",
    "train_labels = [label2id[label] for label in train_labels]\n",
    "\n",
    "dev_text, dev_labels = myutils.read_data(\"data/en_ewt_nn_answers_test.conll\")\n",
    "dev_labels = [label2id[label] for label in dev_labels]\n",
    "\n",
    "print('tokenizing...')\n",
    "tokzr = AutoTokenizer.from_pretrained(MLM)\n",
    "train_tokked = myutils.tok(train_text, tokzr)\n",
    "dev_tokked = myutils.tok(dev_text, tokzr)\n",
    "PAD = tokzr.pad_token_id\n",
    "\n",
    "print('converting to batches...')\n",
    "train_text_batched, train_labels_batched = myutils.to_batch(train_tokked, train_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "# Note, some data is trown away if len(text_tokked)%BATCH_SIZE!= 0\n",
    "dev_text_batched, dev_labels_batched = myutils.to_batch(dev_tokked, dev_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "\n",
    "print('initializing model...')\n",
    "model = ClassModel(NLABELS, MLM)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "print('training...')\n",
    "for epoch in range(EPOCHS):\n",
    "    print('=====================')\n",
    "    print('starting epoch ' + str(epoch))\n",
    "    model.train() \n",
    "\n",
    "    # Loop over batches\n",
    "    loss = 0\n",
    "    for batch_idx in range(0, len(train_text_batched)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_scores = model.forward(train_text_batched[batch_idx])\n",
    "        batch_loss = loss_function(output_scores, train_labels_batched[batch_idx])\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    dev_score = model.run_eval(dev_text_batched, dev_labels_batched)\n",
    "    print('Loss: {:.2f}'.format(loss))\n",
    "    print('Acc(dev): {:.2f}'.format(100*dev_score))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
