{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import generate_masked_sentences from scripts/maskPrecessTest.py\n",
    "from scripts.maskPrecessTest import generate_masked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wnut_17 (/home/malthe/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45235d166f04dcaa2efe7907a241591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")\n",
    "\n",
    "\n",
    "#preprocess data\n",
    "train_data = generate_masked_sentences(wnut['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=32\n",
    "PAD = '<PAD>'\n",
    "\n",
    "word2idx = {PAD:0}\n",
    "idx2word = [PAD]\n",
    "\n",
    "# Generate word2idxs\n",
    "for sentPos, sent in enumerate(train_data):\n",
    "    for wordPos, word in enumerate(sent['tokens'][:max_len]):\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(idx2word)\n",
    "            idx2word.append(word)        \n",
    "\n",
    "# Vocab length\n",
    "vocab_dim = len(idx2word)\n",
    "\n",
    "feats = torch.zeros((len(train_data), max_len), dtype=torch.long)\n",
    "for sentPos, sent in enumerate(train_data):\n",
    "    for wordPos, word in enumerate(sent['tokens'][:max_len]):\n",
    "        wordIdx = word2idx[PAD] if word not in word2idx else word2idx[word]\n",
    "        feats[sentPos][wordPos] = wordIdx\n",
    "\n",
    "# Generate labels as a tensor of booleans indicating if the masked token is a named entity\n",
    "labels = torch.tensor([sent['is_ner'] for sent in train_data], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62730, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.0633123368024826\n",
      "Epoch: 1, Loss: 0.046949632465839386\n",
      "Epoch: 2, Loss: 0.03520117327570915\n",
      "Epoch: 3, Loss: 0.007973277941346169\n",
      "Epoch: 4, Loss: 0.004464718978852034\n",
      "Epoch: 5, Loss: 0.0048362743109464645\n",
      "Epoch: 6, Loss: 0.0018194129224866629\n",
      "Epoch: 7, Loss: 0.003108053235337138\n",
      "Epoch: 8, Loss: 0.0012030262732878327\n",
      "Epoch: 9, Loss: 0.0005174627876840532\n"
     ]
    }
   ],
   "source": [
    "# Define a simple nn model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_dim, emb_dim):\n",
    "        # Model should predict if the masked token is a named entity\n",
    "        super(Model, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_dim, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, 128)\n",
    "\n",
    "        # pool and output a single value\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.output = nn.Linear(128, 1)            \n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.pool(x.transpose(1, 2)).squeeze(2)\n",
    "        x = self.output(x)\n",
    "        # Use sigmoid activation function to get a value between 0 and 1\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "model = Model(vocab_dim, 128)\n",
    "\n",
    "# Define cross entropy loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Train model\n",
    "for epoch in range(10):\n",
    "    for i in range(0, len(feats), batch_size):\n",
    "        batch_feats = feats[i:i+batch_size]\n",
    "        batch_labels = labels[i:i+batch_size]\n",
    "        y_pred = model(batch_feats)\n",
    "        loss = criterion(y_pred, batch_labels.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9109600752329657\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test_data = generate_masked_sentences(wnut['test'])\n",
    "\n",
    "feats = torch.zeros((len(test_data), max_len), dtype=torch.long)\n",
    "for sentPos, sent in enumerate(test_data):\n",
    "    for wordPos, word in enumerate(sent['tokens'][:max_len]):\n",
    "        wordIdx = word2idx[PAD] if word not in word2idx else word2idx[word]\n",
    "        feats[sentPos][wordPos] = wordIdx\n",
    "\n",
    "labels = torch.tensor([sent['is_ner'] for sent in test_data], dtype=torch.float)\n",
    "\n",
    "y_pred = model(feats)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if (y_pred[i] > 0.5) == labels[i]:\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct/len(y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   49.,   392.],\n",
      "        [ 1691., 21262.]])\n"
     ]
    }
   ],
   "source": [
    "# Make confusion matrix\n",
    "confusion_matrix = torch.zeros((2, 2))\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] > 0.5:\n",
    "        if labels[i]:\n",
    "            confusion_matrix[0][0] += 1\n",
    "        else:\n",
    "            confusion_matrix[0][1] += 1\n",
    "    else:\n",
    "        if labels[i]:\n",
    "            confusion_matrix[1][0] += 1\n",
    "        else:\n",
    "            confusion_matrix[1][1] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
