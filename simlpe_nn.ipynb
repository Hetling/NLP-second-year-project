{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import generate_masked_sentences from scripts/maskPrecessTest.py\n",
    "from scripts.maskPrecessTest import generate_masked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wnut_17 (/home/malthe/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda1aeff0ec44d75b840516bb4e85b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")\n",
    "\n",
    "\n",
    "#preprocess data\n",
    "train_data = generate_masked_sentences(wnut['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=32\n",
    "PAD = '<PAD>'\n",
    "\n",
    "word2idx = {PAD:0}\n",
    "idx2word = [PAD]\n",
    "\n",
    "# Generate word2idxs\n",
    "for sentPos, sent in enumerate(train_data):\n",
    "    for wordPos, word in enumerate(sent['tokens'][:max_len]):\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(idx2word)\n",
    "            idx2word.append(word)        \n",
    "\n",
    "# Vocab length\n",
    "vocab_dim = len(idx2word)\n",
    "\n",
    "feats = torch.zeros((len(train_data), max_len), dtype=torch.long)\n",
    "for sentPos, sent in enumerate(train_data):\n",
    "    for wordPos, word in enumerate(sent['tokens'][:max_len]):\n",
    "        wordIdx = word2idx[PAD] if word not in word2idx else word2idx[word]\n",
    "        feats[sentPos][wordPos] = wordIdx\n",
    "\n",
    "# Generate labels as a tensor of booleans indicating if the masked token is a named entity\n",
    "labels = torch.tensor([sent['is_ner'] for sent in train_data], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.0633123368024826\n",
      "Epoch: 1, Loss: 0.046949632465839386\n",
      "Epoch: 2, Loss: 0.03520117327570915\n",
      "Epoch: 3, Loss: 0.007973277941346169\n",
      "Epoch: 4, Loss: 0.004464718978852034\n",
      "Epoch: 5, Loss: 0.0048362743109464645\n",
      "Epoch: 6, Loss: 0.0018194129224866629\n",
      "Epoch: 7, Loss: 0.003108053235337138\n",
      "Epoch: 8, Loss: 0.0012030262732878327\n",
      "Epoch: 9, Loss: 0.0005174627876840532\n"
     ]
    }
   ],
   "source": [
    "# Define a simple nn model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_dim, emb_dim):\n",
    "        # Model should predict if the masked token is a named entity\n",
    "        super(Model, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_dim, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, 128)        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # pool and output a single value\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.mask_output = nn.Linear(128, 1)\n",
    "\n",
    "        # self.linear2 = nn.Linear(128, 128)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x.transpose(1, 2)).squeeze(2)\n",
    "        x = self.mask_output(x)\n",
    "        # Use sigmoid activation function to get a value between 0 and 1\n",
    "        x = torch.sigmoid(x)\n",
    "        # If x is greater than 0.5, then the masked token is a named entity and we run the second linear layer\n",
    "        # x = torch.where(x > 0.5, self.linear2(x), x)\n",
    "        return x\n",
    "    \n",
    "model = Model(vocab_dim, 128)\n",
    "\n",
    "# Define cross entropy loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Train model\n",
    "for epoch in range(10):\n",
    "    for i in range(0, len(feats), batch_size):\n",
    "        batch_feats = feats[i:i+batch_size]\n",
    "        batch_labels = labels[i:i+batch_size]\n",
    "        y_pred = model(batch_feats)\n",
    "        loss = criterion(y_pred, batch_labels.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m         feats[sentPos][wordPos] \u001b[39m=\u001b[39m wordIdx\n\u001b[1;32m     10\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([sent[\u001b[39m'\u001b[39m\u001b[39mis_ner\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m test_data], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m---> 12\u001b[0m y_pred \u001b[39m=\u001b[39m model(feats)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     15\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test_data = generate_masked_sentences(wnut['test'])\n",
    "\n",
    "feats = torch.zeros((len(test_data), max_len), dtype=torch.long)\n",
    "for sentPos, sent in enumerate(test_data):\n",
    "    for wordPos, word in enumerate(sent['tokens'][:max_len]):\n",
    "        wordIdx = word2idx[PAD] if word not in word2idx else word2idx[word]\n",
    "        feats[sentPos][wordPos] = wordIdx\n",
    "\n",
    "labels = torch.tensor([sent['is_ner'] for sent in test_data], dtype=torch.float)\n",
    "\n",
    "y_pred = model(feats)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if (y_pred[i] > 0.5) == labels[i]:\n",
    "        correct += 1\n",
    "print(f'Accuracy: {correct/len(y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   49.,   392.],\n",
      "        [ 1691., 21262.]])\n"
     ]
    }
   ],
   "source": [
    "# Make confusion matrix\n",
    "confusion_matrix = torch.zeros((2, 2))\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] > 0.5:\n",
    "        if labels[i]:\n",
    "            confusion_matrix[0][0] += 1\n",
    "        else:\n",
    "            confusion_matrix[0][1] += 1\n",
    "    else:\n",
    "        if labels[i]:\n",
    "            confusion_matrix[1][0] += 1\n",
    "        else:\n",
    "            confusion_matrix[1][1] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train the second model to detect the named entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by filtering out all the sentences in train_data where the masked token is not a named entity, i.e. the is_ner field is False\n",
    "filtered_train_data = [sent for sent in train_data if sent['is_ner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([3160, 13])\n"
     ]
    }
   ],
   "source": [
    "# Get all ner_tags from wnut_17 dataset\n",
    "ner_tags = wnut['train'].features['ner_tags'].feature.names\n",
    "\n",
    "# Create a dictionary mapping ner_tags to indices\n",
    "idx2ner = {i:ner for i, ner in enumerate(ner_tags)}\n",
    "num_entities = len(idx2ner)\n",
    "print(num_entities)\n",
    "\n",
    "# Create a list of one hot encoded vectors for each sentence in filtered_train_data\n",
    "true_ner_tags = torch.zeros((len(filtered_train_data), num_entities), dtype=torch.float)\n",
    "for sentPos, sent in enumerate(filtered_train_data):\n",
    "    true_ner_tags[sentPos][sent['ner_tag']] = 1\n",
    "\n",
    "\n",
    "feats = torch.zeros((len(filtered_train_data), max_len), dtype=torch.long)\n",
    "for sentPos, sent in enumerate(filtered_train_data):\n",
    "    for wordPos, word in enumerate(sent['tokens'][:max_len]):\n",
    "        wordIdx = word2idx[PAD] if word not in word2idx else word2idx[word]\n",
    "        feats[sentPos][wordPos] = wordIdx\n",
    "\n",
    "print(true_ner_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.475318193435669\n",
      "Epoch: 1, Loss: 2.356250762939453\n",
      "Epoch: 2, Loss: 2.3452768325805664\n",
      "Epoch: 3, Loss: 2.3380355834960938\n",
      "Epoch: 4, Loss: 2.3237128257751465\n",
      "Epoch: 5, Loss: 2.3061845302581787\n",
      "Epoch: 6, Loss: 2.3028838634490967\n",
      "Epoch: 7, Loss: 2.3018319606781006\n",
      "Epoch: 8, Loss: 2.30107045173645\n",
      "Epoch: 9, Loss: 2.315247058868408\n"
     ]
    }
   ],
   "source": [
    "# Create model 2\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, vocab_dim, emb_dim):\n",
    "        # Model should predict the entity type of the masked token\n",
    "        super(Model2, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_dim, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # pool and output a single value\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Output a single value for each entity type\n",
    "        self.output = nn.Linear(128, num_entities)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x.transpose(1, 2)).squeeze(2)\n",
    "        x = self.output(x)        \n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "model2 = Model2(vocab_dim, 128)\n",
    "\n",
    "# Define cross entropy loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train model\n",
    "for epoch in range(10):\n",
    "    for i in range(0, len(feats), batch_size):\n",
    "        batch_feats = feats[i:i+batch_size]\n",
    "        batch_labels = true_ner_tags[i:i+batch_size]\n",
    "        y_pred = model2(batch_feats)        \n",
    "        loss = criterion(y_pred, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.4616e-05, 3.0004e-04, 2.1161e-06, 2.2064e-05, 1.2284e-04, 1.7052e-04,\n",
      "         6.1553e-05, 3.7935e-02, 1.3420e-04, 9.6108e-01, 1.1349e-04, 6.2741e-06,\n",
      "         1.5634e-05]])\n",
      "torch.Size([1, 13])\n",
      "B-person\n"
     ]
    }
   ],
   "source": [
    "# Predict on the sentence 'I live in London'\n",
    "sentence = \"It 's the view from where I 'm living for two weeks . [MASK] State Building\"\n",
    "sentence = \"Hello my name is John and I live in [MASK] State Building\"\n",
    "sentence = sentence.split()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    sentence = torch.tensor([word2idx[word] for word in sentence], dtype=torch.long)\n",
    "    sentence = sentence.unsqueeze(0)\n",
    "    y_pred = model2(sentence)\n",
    "    print(y_pred)\n",
    "    print(y_pred.shape)\n",
    "    print(idx2ner[torch.argmax(y_pred).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-location': 548, 'I-location': 245, 'B-group': 264, 'B-corporation': 221, 'B-person': 660, 'B-creative-work': 140, 'B-product': 142, 'I-person': 335, 'I-creative-work': 206, 'I-corporation': 46, 'I-group': 150, 'I-product': 203}\n"
     ]
    }
   ],
   "source": [
    "# Get class distribution of ner_tags in filtered_train_data\n",
    "ner_counts = {}\n",
    "for sent in filtered_train_data:\n",
    "    ner = idx2ner[sent['ner_tag']]\n",
    "    if ner not in ner_counts:\n",
    "        ner_counts[ner] = 0\n",
    "    ner_counts[ner] += 1\n",
    "\n",
    "print(ner_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now combine the two models into one so we can train them sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 predicts if a masked token is a named entity\n",
    "# Model 2 predicts what type of named entity a masked token is. Model 2 is only run if model 1 predicts that the masked token is a named entity\n",
    "\n",
    "# Define combined model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, vocab_dim, emb_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        # First part of the model is same between the two models\n",
    "        self.word_embeddings = nn.Embedding(vocab_dim, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, 128)                \n",
    "        # Pool together all word embeddings after linear layer\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Model 1 specific layers\n",
    "        self.model1_output = nn.Linear(128, 1)\n",
    "\n",
    "        # Add a gating mechanism to decide whether to run model 2\n",
    "        self.gate = nn.Linear(1, 1)\n",
    "\n",
    "        # Model 2 specific layers        \n",
    "        # Output a single value for whether the masked token is a named entity\n",
    "        self.model2_output = nn.Linear(128, num_entities)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = self.pool(x.transpose(1, 2)).squeeze(2)\n",
    "        # Run through model 1\n",
    "        model1_output = self.model1_output(x)\n",
    "        model1_output = torch.sigmoid(model1_output)\n",
    "\n",
    "        # Run through model 2 if model 1 predicts that the masked token is a named entity\n",
    "        model2_output = self.model2_output(x)\n",
    "        model2_output = torch.softmax(model2_output, dim=1)\n",
    "        # Run through gate\n",
    "        gate_output = self.gate(model1_output)        \n",
    "        gate_output = nn.ReLU(gate_output)\n",
    "        model2_output = model2_output * gate_output\n",
    "        # The gating mechanism should be able to learn to not run model 2 if model 1 predicts that the masked token is not a named entity \n",
    "\n",
    "        return model1_output, model2_output\n",
    "    \n",
    "\n",
    "combined_model = CombinedModel(vocab_dim, 128)\n",
    "\n",
    "criterion1 = nn.BCELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i in range(0, len(feats), batch_size):\n",
    "        x, y_true1, y_true2 = data\n",
    "        optimizer.zero_grad()\n",
    "        y1_pred, y2_pred = model(x.view(x.size(0), -1))\n",
    "        loss1 = criterion1(y1_pred, y_true1.float())\n",
    "        loss2 = criterion2(y2_pred, y_true2)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
